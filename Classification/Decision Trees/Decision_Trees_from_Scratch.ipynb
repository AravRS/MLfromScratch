{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Trees from Scratch",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uh8qkZVkFUG",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m0Rspk6kQ2h",
        "colab_type": "text"
      },
      "source": [
        "#### Necessary Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPt-E0n8kClJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbwvxAqqkU2l",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAzHKIkCkSbr",
        "colab_type": "code",
        "outputId": "b2dab47d-69ee-4d1d-c8a6-a41d0e2dc33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "loans = pd.read_csv(\"lending-club-data.csv\")\n",
        "loans.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>funded_amnt</th>\n",
              "      <th>funded_amnt_inv</th>\n",
              "      <th>term</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>installment</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>emp_title</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>is_inc_v</th>\n",
              "      <th>issue_d</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>pymnt_plan</th>\n",
              "      <th>url</th>\n",
              "      <th>desc</th>\n",
              "      <th>purpose</th>\n",
              "      <th>title</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>addr_state</th>\n",
              "      <th>dti</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>earliest_cr_line</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>initial_list_status</th>\n",
              "      <th>out_prncp</th>\n",
              "      <th>out_prncp_inv</th>\n",
              "      <th>total_pymnt</th>\n",
              "      <th>total_pymnt_inv</th>\n",
              "      <th>total_rec_prncp</th>\n",
              "      <th>total_rec_int</th>\n",
              "      <th>total_rec_late_fee</th>\n",
              "      <th>recoveries</th>\n",
              "      <th>collection_recovery_fee</th>\n",
              "      <th>last_pymnt_d</th>\n",
              "      <th>last_pymnt_amnt</th>\n",
              "      <th>next_pymnt_d</th>\n",
              "      <th>last_credit_pull_d</th>\n",
              "      <th>collections_12_mths_ex_med</th>\n",
              "      <th>mths_since_last_major_derog</th>\n",
              "      <th>policy_code</th>\n",
              "      <th>not_compliant</th>\n",
              "      <th>status</th>\n",
              "      <th>inactive_loans</th>\n",
              "      <th>bad_loans</th>\n",
              "      <th>emp_length_num</th>\n",
              "      <th>grade_num</th>\n",
              "      <th>sub_grade_num</th>\n",
              "      <th>delinq_2yrs_zero</th>\n",
              "      <th>pub_rec_zero</th>\n",
              "      <th>collections_12_mths_zero</th>\n",
              "      <th>short_emp</th>\n",
              "      <th>payment_inc_ratio</th>\n",
              "      <th>final_d</th>\n",
              "      <th>last_delinq_none</th>\n",
              "      <th>last_record_none</th>\n",
              "      <th>last_major_derog_none</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1077501</td>\n",
              "      <td>1296599</td>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "      <td>4975</td>\n",
              "      <td>36 months</td>\n",
              "      <td>10.65</td>\n",
              "      <td>162.87</td>\n",
              "      <td>B</td>\n",
              "      <td>B2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>24000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>20111201T000000</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td>Borrower added on 12/22/11 &gt; I need to upgra...</td>\n",
              "      <td>credit_card</td>\n",
              "      <td>Computer</td>\n",
              "      <td>860xx</td>\n",
              "      <td>AZ</td>\n",
              "      <td>27.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19850101T000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13648</td>\n",
              "      <td>83.7</td>\n",
              "      <td>9.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5861.07</td>\n",
              "      <td>5831.78</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>861.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>171.62</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.14350</td>\n",
              "      <td>20141201T000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1077430</td>\n",
              "      <td>1314167</td>\n",
              "      <td>2500</td>\n",
              "      <td>2500</td>\n",
              "      <td>2500</td>\n",
              "      <td>60 months</td>\n",
              "      <td>15.27</td>\n",
              "      <td>59.83</td>\n",
              "      <td>C</td>\n",
              "      <td>C4</td>\n",
              "      <td>Ryder</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>RENT</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>20111201T000000</td>\n",
              "      <td>Charged Off</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td>Borrower added on 12/22/11 &gt; I plan to use t...</td>\n",
              "      <td>car</td>\n",
              "      <td>bike</td>\n",
              "      <td>309xx</td>\n",
              "      <td>GA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19990401T000000</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1687</td>\n",
              "      <td>9.4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1008.71</td>\n",
              "      <td>1008.71</td>\n",
              "      <td>456.46</td>\n",
              "      <td>435.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>117.08</td>\n",
              "      <td>1.11</td>\n",
              "      <td>20130401T000000</td>\n",
              "      <td>119.66</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20130901T000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Charged Off</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.39320</td>\n",
              "      <td>20161201T000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1077175</td>\n",
              "      <td>1313524</td>\n",
              "      <td>2400</td>\n",
              "      <td>2400</td>\n",
              "      <td>2400</td>\n",
              "      <td>36 months</td>\n",
              "      <td>15.96</td>\n",
              "      <td>84.33</td>\n",
              "      <td>C</td>\n",
              "      <td>C5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>12252.0</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>20111201T000000</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>small_business</td>\n",
              "      <td>real estate business</td>\n",
              "      <td>606xx</td>\n",
              "      <td>IL</td>\n",
              "      <td>8.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20011101T000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2956</td>\n",
              "      <td>98.5</td>\n",
              "      <td>10.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3003.65</td>\n",
              "      <td>3003.65</td>\n",
              "      <td>2400.00</td>\n",
              "      <td>603.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20140601T000000</td>\n",
              "      <td>649.91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150201T000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.25955</td>\n",
              "      <td>20141201T000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1076863</td>\n",
              "      <td>1277178</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000</td>\n",
              "      <td>10000</td>\n",
              "      <td>36 months</td>\n",
              "      <td>13.49</td>\n",
              "      <td>339.31</td>\n",
              "      <td>C</td>\n",
              "      <td>C1</td>\n",
              "      <td>AIR RESOURCES BOARD</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>49200.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>20111201T000000</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td>Borrower added on 12/21/11 &gt; to pay for prop...</td>\n",
              "      <td>other</td>\n",
              "      <td>personel</td>\n",
              "      <td>917xx</td>\n",
              "      <td>CA</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19960201T000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5598</td>\n",
              "      <td>21.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12226.30</td>\n",
              "      <td>12226.30</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>2209.33</td>\n",
              "      <td>16.97</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>357.48</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.27585</td>\n",
              "      <td>20141201T000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1075269</td>\n",
              "      <td>1311441</td>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "      <td>5000</td>\n",
              "      <td>36 months</td>\n",
              "      <td>7.90</td>\n",
              "      <td>156.46</td>\n",
              "      <td>A</td>\n",
              "      <td>A4</td>\n",
              "      <td>Veolia Transportaton</td>\n",
              "      <td>3 years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>36000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>20111201T000000</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td>NaN</td>\n",
              "      <td>wedding</td>\n",
              "      <td>My wedding loan I promise to pay back</td>\n",
              "      <td>852xx</td>\n",
              "      <td>AZ</td>\n",
              "      <td>11.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20041101T000000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7963</td>\n",
              "      <td>28.3</td>\n",
              "      <td>12.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5631.38</td>\n",
              "      <td>5631.38</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>631.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>161.03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150201T000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.21533</td>\n",
              "      <td>20141201T000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  member_id  ...  last_record_none  last_major_derog_none\n",
              "0  1077501    1296599  ...                 1                      1\n",
              "1  1077430    1314167  ...                 1                      1\n",
              "2  1077175    1313524  ...                 1                      1\n",
              "3  1076863    1277178  ...                 1                      1\n",
              "4  1075269    1311441  ...                 1                      1\n",
              "\n",
              "[5 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY7OyvshnoSg",
        "colab_type": "code",
        "outputId": "d1f2cb6a-17bc-4a87-be58-12c9545a1b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loans.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 122607 entries, 0 to 122606\n",
            "Data columns (total 68 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   id                           122607 non-null  int64  \n",
            " 1   member_id                    122607 non-null  int64  \n",
            " 2   loan_amnt                    122607 non-null  int64  \n",
            " 3   funded_amnt                  122607 non-null  int64  \n",
            " 4   funded_amnt_inv              122607 non-null  int64  \n",
            " 5   term                         122607 non-null  object \n",
            " 6   int_rate                     122607 non-null  float64\n",
            " 7   installment                  122607 non-null  float64\n",
            " 8   grade                        122607 non-null  object \n",
            " 9   sub_grade                    122607 non-null  object \n",
            " 10  emp_title                    115767 non-null  object \n",
            " 11  emp_length                   118516 non-null  object \n",
            " 12  home_ownership               122607 non-null  object \n",
            " 13  annual_inc                   122603 non-null  float64\n",
            " 14  is_inc_v                     122607 non-null  object \n",
            " 15  issue_d                      122607 non-null  object \n",
            " 16  loan_status                  122607 non-null  object \n",
            " 17  pymnt_plan                   122607 non-null  object \n",
            " 18  url                          122607 non-null  object \n",
            " 19  desc                         60705 non-null   object \n",
            " 20  purpose                      122607 non-null  object \n",
            " 21  title                        122596 non-null  object \n",
            " 22  zip_code                     122607 non-null  object \n",
            " 23  addr_state                   122607 non-null  object \n",
            " 24  dti                          122607 non-null  float64\n",
            " 25  delinq_2yrs                  122578 non-null  float64\n",
            " 26  earliest_cr_line             122578 non-null  object \n",
            " 27  inq_last_6mths               122578 non-null  float64\n",
            " 28  mths_since_last_delinq       50500 non-null   float64\n",
            " 29  mths_since_last_record       12531 non-null   float64\n",
            " 30  open_acc                     122578 non-null  float64\n",
            " 31  pub_rec                      122578 non-null  float64\n",
            " 32  revol_bal                    122607 non-null  int64  \n",
            " 33  revol_util                   122607 non-null  float64\n",
            " 34  total_acc                    122578 non-null  float64\n",
            " 35  initial_list_status          122607 non-null  object \n",
            " 36  out_prncp                    122607 non-null  float64\n",
            " 37  out_prncp_inv                122607 non-null  float64\n",
            " 38  total_pymnt                  122607 non-null  float64\n",
            " 39  total_pymnt_inv              122607 non-null  float64\n",
            " 40  total_rec_prncp              122607 non-null  float64\n",
            " 41  total_rec_int                122607 non-null  float64\n",
            " 42  total_rec_late_fee           122607 non-null  float64\n",
            " 43  recoveries                   122607 non-null  float64\n",
            " 44  collection_recovery_fee      122607 non-null  float64\n",
            " 45  last_pymnt_d                 122271 non-null  object \n",
            " 46  last_pymnt_amnt              122607 non-null  float64\n",
            " 47  next_pymnt_d                 2907 non-null    object \n",
            " 48  last_credit_pull_d           122601 non-null  object \n",
            " 49  collections_12_mths_ex_med   122462 non-null  float64\n",
            " 50  mths_since_last_major_derog  15460 non-null   float64\n",
            " 51  policy_code                  122607 non-null  int64  \n",
            " 52  not_compliant                122607 non-null  int64  \n",
            " 53  status                       122607 non-null  object \n",
            " 54  inactive_loans               122607 non-null  int64  \n",
            " 55  bad_loans                    122607 non-null  int64  \n",
            " 56  emp_length_num               122607 non-null  int64  \n",
            " 57  grade_num                    122607 non-null  int64  \n",
            " 58  sub_grade_num                122607 non-null  float64\n",
            " 59  delinq_2yrs_zero             122578 non-null  float64\n",
            " 60  pub_rec_zero                 122578 non-null  float64\n",
            " 61  collections_12_mths_zero     122462 non-null  float64\n",
            " 62  short_emp                    122607 non-null  int64  \n",
            " 63  payment_inc_ratio            122603 non-null  float64\n",
            " 64  final_d                      122607 non-null  object \n",
            " 65  last_delinq_none             122607 non-null  int64  \n",
            " 66  last_record_none             122607 non-null  int64  \n",
            " 67  last_major_derog_none        122607 non-null  int64  \n",
            "dtypes: float64(29), int64(16), object(23)\n",
            "memory usage: 63.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1IqcPiunsof",
        "colab_type": "code",
        "outputId": "31c8cd67-d272-4794-c550-9d1ec0c415dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "loans.describe(include='all')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>member_id</th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>funded_amnt</th>\n",
              "      <th>funded_amnt_inv</th>\n",
              "      <th>term</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>installment</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>emp_title</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>is_inc_v</th>\n",
              "      <th>issue_d</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>pymnt_plan</th>\n",
              "      <th>url</th>\n",
              "      <th>desc</th>\n",
              "      <th>purpose</th>\n",
              "      <th>title</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>addr_state</th>\n",
              "      <th>dti</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>earliest_cr_line</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>initial_list_status</th>\n",
              "      <th>out_prncp</th>\n",
              "      <th>out_prncp_inv</th>\n",
              "      <th>total_pymnt</th>\n",
              "      <th>total_pymnt_inv</th>\n",
              "      <th>total_rec_prncp</th>\n",
              "      <th>total_rec_int</th>\n",
              "      <th>total_rec_late_fee</th>\n",
              "      <th>recoveries</th>\n",
              "      <th>collection_recovery_fee</th>\n",
              "      <th>last_pymnt_d</th>\n",
              "      <th>last_pymnt_amnt</th>\n",
              "      <th>next_pymnt_d</th>\n",
              "      <th>last_credit_pull_d</th>\n",
              "      <th>collections_12_mths_ex_med</th>\n",
              "      <th>mths_since_last_major_derog</th>\n",
              "      <th>policy_code</th>\n",
              "      <th>not_compliant</th>\n",
              "      <th>status</th>\n",
              "      <th>inactive_loans</th>\n",
              "      <th>bad_loans</th>\n",
              "      <th>emp_length_num</th>\n",
              "      <th>grade_num</th>\n",
              "      <th>sub_grade_num</th>\n",
              "      <th>delinq_2yrs_zero</th>\n",
              "      <th>pub_rec_zero</th>\n",
              "      <th>collections_12_mths_zero</th>\n",
              "      <th>short_emp</th>\n",
              "      <th>payment_inc_ratio</th>\n",
              "      <th>final_d</th>\n",
              "      <th>last_delinq_none</th>\n",
              "      <th>last_record_none</th>\n",
              "      <th>last_major_derog_none</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.226070e+05</td>\n",
              "      <td>1.226070e+05</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>115767</td>\n",
              "      <td>118516</td>\n",
              "      <td>122607</td>\n",
              "      <td>1.226030e+05</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>60705</td>\n",
              "      <td>122607</td>\n",
              "      <td>122596</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>122578</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>50500.000000</td>\n",
              "      <td>12531.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>1.226070e+05</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122271</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>2907</td>\n",
              "      <td>122601</td>\n",
              "      <td>122462.000000</td>\n",
              "      <td>15460.000000</td>\n",
              "      <td>122607.0</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607.0</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>122578.000000</td>\n",
              "      <td>122462.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122603.000000</td>\n",
              "      <td>122607</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "      <td>122607.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>35</td>\n",
              "      <td>77167</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>91</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>122607</td>\n",
              "      <td>60314</td>\n",
              "      <td>12</td>\n",
              "      <td>36602</td>\n",
              "      <td>865</td>\n",
              "      <td>50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>597</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>87</td>\n",
              "      <td>NaN</td>\n",
              "      <td>89</td>\n",
              "      <td>92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>115</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36 months</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>B</td>\n",
              "      <td>B3</td>\n",
              "      <td>Teacher</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>MORTGAGE</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Verified</td>\n",
              "      <td>20130801T000000</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>https://www.lendingclub.com/browse/loanDetail....</td>\n",
              "      <td></td>\n",
              "      <td>debt_consolidation</td>\n",
              "      <td>Debt consolidation</td>\n",
              "      <td>945xx</td>\n",
              "      <td>CA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20001001T000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>f</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150101T000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20150301T000000</td>\n",
              "      <td>20150201T000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20160501T000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>97801</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37172</td>\n",
              "      <td>9036</td>\n",
              "      <td>458</td>\n",
              "      <td>34100</td>\n",
              "      <td>59240</td>\n",
              "      <td>NaN</td>\n",
              "      <td>46090</td>\n",
              "      <td>3493</td>\n",
              "      <td>97544</td>\n",
              "      <td>122607</td>\n",
              "      <td>1</td>\n",
              "      <td>227</td>\n",
              "      <td>68233</td>\n",
              "      <td>17773</td>\n",
              "      <td>1612</td>\n",
              "      <td>21637</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1103</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102225</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131</td>\n",
              "      <td>36203</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3102</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>4.728452e+06</td>\n",
              "      <td>5.493222e+06</td>\n",
              "      <td>12809.733743</td>\n",
              "      <td>12736.123753</td>\n",
              "      <td>12497.828395</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.639487</td>\n",
              "      <td>396.623285</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.138502e+04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.496888</td>\n",
              "      <td>0.211996</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.980861</td>\n",
              "      <td>35.258198</td>\n",
              "      <td>73.964488</td>\n",
              "      <td>10.457749</td>\n",
              "      <td>0.103477</td>\n",
              "      <td>1.483507e+04</td>\n",
              "      <td>53.716307</td>\n",
              "      <td>24.248169</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.028899</td>\n",
              "      <td>22.015354</td>\n",
              "      <td>12510.211529</td>\n",
              "      <td>12269.338465</td>\n",
              "      <td>10707.864970</td>\n",
              "      <td>1696.232305</td>\n",
              "      <td>0.742344</td>\n",
              "      <td>105.371929</td>\n",
              "      <td>4.957537</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6156.808616</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.003495</td>\n",
              "      <td>42.428913</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.021761</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.188815</td>\n",
              "      <td>6.370256</td>\n",
              "      <td>4.232882</td>\n",
              "      <td>0.597509</td>\n",
              "      <td>0.858107</td>\n",
              "      <td>0.908173</td>\n",
              "      <td>0.996734</td>\n",
              "      <td>0.123672</td>\n",
              "      <td>7.564725</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.588115</td>\n",
              "      <td>0.897795</td>\n",
              "      <td>0.873906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.938517e+06</td>\n",
              "      <td>6.604693e+06</td>\n",
              "      <td>7932.313398</td>\n",
              "      <td>7887.167118</td>\n",
              "      <td>7946.731527</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.390836</td>\n",
              "      <td>239.475936</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.841483e+04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.497442</td>\n",
              "      <td>0.662052</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.263588</td>\n",
              "      <td>21.950074</td>\n",
              "      <td>35.855540</td>\n",
              "      <td>4.713957</td>\n",
              "      <td>0.359382</td>\n",
              "      <td>1.937773e+04</td>\n",
              "      <td>25.723881</td>\n",
              "      <td>11.629814</td>\n",
              "      <td>NaN</td>\n",
              "      <td>611.510973</td>\n",
              "      <td>611.116057</td>\n",
              "      <td>9046.331429</td>\n",
              "      <td>9061.593794</td>\n",
              "      <td>8041.776401</td>\n",
              "      <td>1793.448627</td>\n",
              "      <td>5.363268</td>\n",
              "      <td>575.651981</td>\n",
              "      <td>94.427007</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7155.007894</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.063674</td>\n",
              "      <td>21.457282</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.145901</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.391363</td>\n",
              "      <td>3.736014</td>\n",
              "      <td>1.362138</td>\n",
              "      <td>0.278934</td>\n",
              "      <td>0.348942</td>\n",
              "      <td>0.288783</td>\n",
              "      <td>0.057059</td>\n",
              "      <td>0.329208</td>\n",
              "      <td>4.127291</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.492177</td>\n",
              "      <td>0.302918</td>\n",
              "      <td>0.331957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.473400e+04</td>\n",
              "      <td>7.047300e+04</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.420000</td>\n",
              "      <td>15.670000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.896000e+03</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028895</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.532925e+05</td>\n",
              "      <td>1.064872e+06</td>\n",
              "      <td>6700.000000</td>\n",
              "      <td>6625.000000</td>\n",
              "      <td>6271.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.620000</td>\n",
              "      <td>215.985000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.405250e+04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.880000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.474000e+03</td>\n",
              "      <td>34.800000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5669.805000</td>\n",
              "      <td>5454.050000</td>\n",
              "      <td>4500.000000</td>\n",
              "      <td>550.555000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>440.180000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.362575</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.621401e+06</td>\n",
              "      <td>1.879659e+06</td>\n",
              "      <td>11000.000000</td>\n",
              "      <td>10950.000000</td>\n",
              "      <td>10500.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.480000</td>\n",
              "      <td>348.180000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.000000e+04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.260000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.058600e+04</td>\n",
              "      <td>55.700000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10575.400000</td>\n",
              "      <td>10352.500000</td>\n",
              "      <td>9100.000000</td>\n",
              "      <td>1136.920000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3609.580000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.965760</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.699942e+06</td>\n",
              "      <td>8.117948e+06</td>\n",
              "      <td>17600.000000</td>\n",
              "      <td>17425.000000</td>\n",
              "      <td>17000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16.290000</td>\n",
              "      <td>521.930000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.500000e+04</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>20.850000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.855200e+04</td>\n",
              "      <td>74.300000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>17177.650000</td>\n",
              "      <td>16914.050000</td>\n",
              "      <td>15000.000000</td>\n",
              "      <td>2167.950000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9554.700000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.215850</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.784128e+07</td>\n",
              "      <td>4.060424e+07</td>\n",
              "      <td>35000.000000</td>\n",
              "      <td>35000.000000</td>\n",
              "      <td>35000.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.060000</td>\n",
              "      <td>1408.130000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.141778e+06</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>39.880000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>152.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1.743266e+06</td>\n",
              "      <td>150.700000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>34310.200000</td>\n",
              "      <td>34310.200000</td>\n",
              "      <td>55758.200000</td>\n",
              "      <td>53913.900000</td>\n",
              "      <td>35000.000000</td>\n",
              "      <td>20758.200000</td>\n",
              "      <td>208.820000</td>\n",
              "      <td>29282.100000</td>\n",
              "      <td>7002.190000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>36234.400000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>152.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>54.171000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id     member_id  ...  last_record_none  last_major_derog_none\n",
              "count   1.226070e+05  1.226070e+05  ...     122607.000000          122607.000000\n",
              "unique           NaN           NaN  ...               NaN                    NaN\n",
              "top              NaN           NaN  ...               NaN                    NaN\n",
              "freq             NaN           NaN  ...               NaN                    NaN\n",
              "mean    4.728452e+06  5.493222e+06  ...          0.897795               0.873906\n",
              "std     5.938517e+06  6.604693e+06  ...          0.302918               0.331957\n",
              "min     5.473400e+04  7.047300e+04  ...          0.000000               0.000000\n",
              "25%     8.532925e+05  1.064872e+06  ...          1.000000               1.000000\n",
              "50%     1.621401e+06  1.879659e+06  ...          1.000000               1.000000\n",
              "75%     6.699942e+06  8.117948e+06  ...          1.000000               1.000000\n",
              "max     3.784128e+07  4.060424e+07  ...          1.000000               1.000000\n",
              "\n",
              "[11 rows x 68 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFKncGIBnlI0",
        "colab_type": "text"
      },
      "source": [
        "**Assigning the Labels:** +1 for a safe loan, and -1 for a unsafe loan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZV6gqitnSXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
        "loans = loans.drop('bad_loans', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ1xC2VQn8mI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = ['grade', 'term', 'home_ownership', 'emp_length']\n",
        "target = 'safe_loans'\n",
        "loans = loans[features + [target]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ypmkzBqnykZ",
        "colab_type": "code",
        "outputId": "7514cde9-6f2f-4dc8-9241-1e1c7a3ee72b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "loans.head(10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade</th>\n",
              "      <th>term</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>safe_loans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>B</td>\n",
              "      <td>36 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C</td>\n",
              "      <td>60 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>C</td>\n",
              "      <td>36 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>C</td>\n",
              "      <td>36 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>36 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>3 years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>E</td>\n",
              "      <td>36 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>9 years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>F</td>\n",
              "      <td>60 months</td>\n",
              "      <td>OWN</td>\n",
              "      <td>4 years</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>B</td>\n",
              "      <td>60 months</td>\n",
              "      <td>RENT</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>C</td>\n",
              "      <td>60 months</td>\n",
              "      <td>OWN</td>\n",
              "      <td>5 years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>B</td>\n",
              "      <td>36 months</td>\n",
              "      <td>OWN</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  grade        term home_ownership emp_length  safe_loans\n",
              "0     B   36 months           RENT  10+ years           1\n",
              "1     C   60 months           RENT   < 1 year          -1\n",
              "2     C   36 months           RENT  10+ years           1\n",
              "3     C   36 months           RENT  10+ years           1\n",
              "4     A   36 months           RENT    3 years           1\n",
              "5     E   36 months           RENT    9 years           1\n",
              "6     F   60 months            OWN    4 years          -1\n",
              "7     B   60 months           RENT   < 1 year          -1\n",
              "8     C   60 months            OWN    5 years           1\n",
              "9     B   36 months            OWN  10+ years           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtDvpUUNtGIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loans = loans.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2CIIwicoCSL",
        "colab_type": "code",
        "outputId": "eb4e0c42-34a5-4657-c8ca-f447050372c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "safe_loans_raw = loans[loans[target] == 1]\n",
        "risky_loans_raw = loans[loans[target] == -1]\n",
        "print(\"Percentage of safe loans                 :\", len(safe_loans_raw) / float(len(loans)))\n",
        "print(\"Percentage of risky loans                :\", len(risky_loans_raw) / float(len(loans)))\n",
        "print(\"Total number of loans in our old dataset :\", len(loans))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of safe loans                 : 0.8111853319957262\n",
            "Percentage of risky loans                : 0.18881466800427382\n",
            "Total number of loans in our old dataset : 122607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bW0gDF_oPnO",
        "colab_type": "code",
        "outputId": "08102515-b67a-4110-ac57-f0a707773c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Since there are less risky loans than safe loans, we make them equal.\n",
        "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
        "safe_loans = safe_loans_raw.iloc[:int(percentage*len(safe_loans_raw))]\n",
        "risky_loans = risky_loans_raw\n",
        "loans_data = risky_loans.append(safe_loans)\n",
        "\n",
        "print(\"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data)))\n",
        "print(\"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data)))\n",
        "print(\"Total number of loans in our new dataset :\", len(loans_data))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of safe loans                 : 0.5\n",
            "Percentage of risky loans                : 0.5\n",
            "Total number of loans in our new dataset : 46300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-mARN3n8-P",
        "colab_type": "text"
      },
      "source": [
        "**1-Hot Encoding:** Converting Categorical Features to a Binary Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTXky3exo_Lu",
        "colab_type": "code",
        "outputId": "b81d2297-782b-47ae-9620-fc3172f4244c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "loans_data = pd.get_dummies(loans_data)\n",
        "loans_data.head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>safe_loans</th>\n",
              "      <th>grade_A</th>\n",
              "      <th>grade_B</th>\n",
              "      <th>grade_C</th>\n",
              "      <th>grade_D</th>\n",
              "      <th>grade_E</th>\n",
              "      <th>grade_F</th>\n",
              "      <th>grade_G</th>\n",
              "      <th>term_ 36 months</th>\n",
              "      <th>term_ 60 months</th>\n",
              "      <th>home_ownership_MORTGAGE</th>\n",
              "      <th>home_ownership_OTHER</th>\n",
              "      <th>home_ownership_OWN</th>\n",
              "      <th>home_ownership_RENT</th>\n",
              "      <th>emp_length_0</th>\n",
              "      <th>emp_length_1 year</th>\n",
              "      <th>emp_length_10+ years</th>\n",
              "      <th>emp_length_2 years</th>\n",
              "      <th>emp_length_3 years</th>\n",
              "      <th>emp_length_4 years</th>\n",
              "      <th>emp_length_5 years</th>\n",
              "      <th>emp_length_6 years</th>\n",
              "      <th>emp_length_7 years</th>\n",
              "      <th>emp_length_8 years</th>\n",
              "      <th>emp_length_9 years</th>\n",
              "      <th>emp_length_&lt; 1 year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    safe_loans  grade_A  ...  emp_length_9 years  emp_length_< 1 year\n",
              "1           -1        0  ...                   0                    1\n",
              "6           -1        0  ...                   0                    0\n",
              "7           -1        0  ...                   0                    1\n",
              "10          -1        0  ...                   0                    1\n",
              "12          -1        0  ...                   0                    0\n",
              "18          -1        0  ...                   0                    0\n",
              "21          -1        0  ...                   0                    0\n",
              "23          -1        0  ...                   1                    0\n",
              "24          -1        0  ...                   0                    0\n",
              "41          -1        1  ...                   0                    0\n",
              "\n",
              "[10 rows x 26 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXxumlLXrBLl",
        "colab_type": "code",
        "outputId": "a49ea442-9fe4-44b2-934f-81049a658cd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "features = loans_data.drop('safe_loans', axis=1)\n",
        "features.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>grade_A</th>\n",
              "      <th>grade_B</th>\n",
              "      <th>grade_C</th>\n",
              "      <th>grade_D</th>\n",
              "      <th>grade_E</th>\n",
              "      <th>grade_F</th>\n",
              "      <th>grade_G</th>\n",
              "      <th>term_ 36 months</th>\n",
              "      <th>term_ 60 months</th>\n",
              "      <th>home_ownership_MORTGAGE</th>\n",
              "      <th>home_ownership_OTHER</th>\n",
              "      <th>home_ownership_OWN</th>\n",
              "      <th>home_ownership_RENT</th>\n",
              "      <th>emp_length_0</th>\n",
              "      <th>emp_length_1 year</th>\n",
              "      <th>emp_length_10+ years</th>\n",
              "      <th>emp_length_2 years</th>\n",
              "      <th>emp_length_3 years</th>\n",
              "      <th>emp_length_4 years</th>\n",
              "      <th>emp_length_5 years</th>\n",
              "      <th>emp_length_6 years</th>\n",
              "      <th>emp_length_7 years</th>\n",
              "      <th>emp_length_8 years</th>\n",
              "      <th>emp_length_9 years</th>\n",
              "      <th>emp_length_&lt; 1 year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    grade_A  grade_B  ...  emp_length_9 years  emp_length_< 1 year\n",
              "1         0        0  ...                   0                    1\n",
              "6         0        0  ...                   0                    0\n",
              "7         0        1  ...                   0                    1\n",
              "10        0        0  ...                   0                    1\n",
              "12        0        1  ...                   0                    0\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciYY5OoirPn2",
        "colab_type": "code",
        "outputId": "25130f4e-6df6-42c8-b8f3-ddf2f36e8fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f\"Number of features (after binarizing categorical variables) = {len(features.columns)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features (after binarizing categorical variables) = 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zCy-kuXICS",
        "colab_type": "text"
      },
      "source": [
        "Loading train/test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T90l540dRiFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "with open('train-idx.json', 'r') as f:\n",
        "    train_idx = json.load(f)\n",
        "with open('test-idx.json', 'r') as f:\n",
        "    test_idx = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_MD2cwRscrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data =  pd.get_dummies(loans.iloc[train_idx])\n",
        "test_data =  pd.get_dummies(loans.iloc[test_idx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwB8k1OnXOZX",
        "colab_type": "code",
        "outputId": "11796412-d81d-433c-807f-30fe2408d005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(train_data), len(test_data))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37224 9284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyw0g3Dbtp0O",
        "colab_type": "text"
      },
      "source": [
        "## Decision Tree Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtK9fzaiLlkr",
        "colab_type": "text"
      },
      "source": [
        "**Count number of mistakes while predicting majority class.**\n",
        "\n",
        "1. Calculate the number of safe loans and risky loans.\n",
        "2. Since we are assuming majority class prediction, all the data points that are not in the majority class are considered mistakes.\n",
        "3. Return the number of mistakes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoecqFJ6tmgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def intermediate_node_num_mistakes(labels_in_node):\n",
        "    \n",
        "    if len(labels_in_node) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # Count the number of 1's and -1's\n",
        "    num_of_positive = (labels_in_node == +1).sum() \n",
        "    num_of_negative = (labels_in_node == -1).sum()\n",
        "                \n",
        "    # Return the number of mistakes that the majority classifier makes.\n",
        "    return num_of_negative if num_of_positive > num_of_negative else num_of_positive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjgWQF-CMIWc",
        "colab_type": "text"
      },
      "source": [
        "**Pick best feature to split on**\n",
        "\n",
        "1. Loop over each feature in the feature list.\n",
        "2. Within the loop, split the data into two groups: one group where all of the data has feature value 0 or False *(left split)*, and one group where all of the data has feature value 1 or True *(right split)*.\n",
        "3. Calculate the number of misclassified examples in both groups of data and compute the classification error.\n",
        "4. If the computed error is smaller than the best error found so far, store this feature and its error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKteqzgxuQ1f",
        "colab_type": "text"
      },
      "source": [
        "$ classification \\ error=  \\dfrac{number \\ of \\ mistakes}{total \\ number \\ of \\ examples} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkHtYByTt0Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_splitting_feature(data, features, target):\n",
        "    \n",
        "    best_feature = None  \n",
        "    best_error = 10\n",
        "    num_data_points = float(len(data))    \n",
        "\n",
        "    for feature in features:\n",
        "        \n",
        "        # The left split will have all data points where the feature value is 0 and 1\n",
        "        left_split = data[data[feature] == 0]\n",
        "        right_split =  data[data[feature] == 1]\n",
        "            \n",
        "        # Calculate the number of misclassified examples.\n",
        "        left_mistakes = intermediate_node_num_mistakes(left_split[target])            \n",
        "        right_mistakes = intermediate_node_num_mistakes(right_split[target])\n",
        "            \n",
        "        # Compute the classification error of this split.\n",
        "        error = (left_mistakes + right_mistakes) / num_data_points\n",
        "\n",
        "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
        "        if error < best_error:\n",
        "            best_feature = feature            \n",
        "            best_error = error              \n",
        "    \n",
        "    return best_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUheljEOvj62",
        "colab_type": "text"
      },
      "source": [
        "#### Building the Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0-xVdp3Mjxt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "{ \n",
        "   'is_leaf'            : True/False.\n",
        "   'prediction'         : Prediction at the leaf node.\n",
        "   'left'               : (dictionary corresponding to the left tree).\n",
        "   'right'              : (dictionary corresponding to the right tree).\n",
        "   'splitting_feature'  : The feature that this node splits on.\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw_sh45yuscy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_leaf(target_values):\n",
        "    \n",
        "    # Create a leaf node\n",
        "    leaf = {'splitting_feature' : None,\n",
        "            'left' : None,\n",
        "            'right' : None,\n",
        "            'is_leaf':  True  }\n",
        "    \n",
        "    # Count the number of data points that are +1 and -1 in this node.\n",
        "    num_plus_ones = len(target_values[target_values == +1])\n",
        "    num_minus_ones = len(target_values[target_values == -1])\n",
        "    \n",
        "    # For the leaf node, set the prediction to be the majority class.\n",
        "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
        "    if num_plus_ones > num_minus_ones:\n",
        "        leaf['prediction'] = +1\n",
        "    else:\n",
        "        leaf['prediction'] =  -1\n",
        "              \n",
        "    return leaf "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD4kBdg5v4Ko",
        "colab_type": "text"
      },
      "source": [
        "**Stopping conditions:**\n",
        "\n",
        "1. All data points in a node are from the same class.\n",
        "2. No more features to split on.\n",
        "3. Limit tree depth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIag24yVvlIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decision_tree_create(data, features, target, current_depth = 0, max_depth = 10):\n",
        "    remaining_features = features.copy() # Make a copy of the features.\n",
        "    \n",
        "    target_values = data[target]\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(f\"Subtree, depth = {current_depth} ({len(target_values)} data points).\")\n",
        "    \n",
        "\n",
        "    # Stopping condition 1. Error is 0.\n",
        "    if intermediate_node_num_mistakes(target_values) == 0:\n",
        "        print(\"Stopping condition 1 reached.\")  \n",
        "        return create_leaf(target_values)\n",
        "    \n",
        "    # Stopping condition 2. No more features.\n",
        "    if remaining_features.empty:\n",
        "        print(\"Stopping condition 2 reached.\")    \n",
        "        return create_leaf(target_values)    \n",
        "    \n",
        "    # Stopping condition 3. Limit tree depth.\n",
        "    if current_depth >= max_depth:\n",
        "        print(\"Reached maximum depth. Stopping for now.\")\n",
        "        return create_leaf(target_values)\n",
        "\n",
        "    # Find the best splitting feature\n",
        "    splitting_feature = best_splitting_feature(data, features, target)\n",
        "    \n",
        "    # Split on the best feature \n",
        "    left_split = data[data[splitting_feature] == 0]\n",
        "    right_split = data[data[splitting_feature] == 1]\n",
        "    remaining_features.drop(splitting_feature, axis=1, inplace=True)\n",
        "    print(f\"Split on feature {splitting_feature}. ({len(left_split)}, {len(right_split)})\")\n",
        "    \n",
        "    # Create a leaf node if the split is perfect\n",
        "    if len(left_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf(left_split[target])\n",
        "    if len(right_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf(right_split[target])\n",
        "        \n",
        "    # Repeat (recurse) on left and right subtrees\n",
        "    left_tree = decision_tree_create(left_split, remaining_features, target, current_depth + 1, max_depth)        \n",
        "    right_tree = decision_tree_create(right_split, remaining_features, target, current_depth + 1, max_depth) \n",
        "\n",
        "    return {'is_leaf'          : False, \n",
        "            'prediction'       : None,\n",
        "            'splitting_feature': splitting_feature,\n",
        "            'left'             : left_tree, \n",
        "            'right'            : right_tree}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE8h8jhXwD2f",
        "colab_type": "text"
      },
      "source": [
        "**Function to Count the Nodes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPcFeDocvy8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_nodes(tree):\n",
        "    if tree['is_leaf']:\n",
        "        return 1\n",
        "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_kQrTl5wjuc",
        "colab_type": "text"
      },
      "source": [
        "#### Fit the Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6hzBRkpwjZ-",
        "colab_type": "code",
        "outputId": "6ad6d10e-b1f2-41a2-bc80-d976657cd7c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "my_decision_tree = decision_tree_create(train_data, features, 'safe_loans', max_depth = 6)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 0 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (9223 data points).\n",
            "Split on feature grade_A. (9122, 101)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9122 data points).\n",
            "Split on feature grade_B. (8074, 1048)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (8074 data points).\n",
            "Split on feature grade_C. (5884, 2190)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (5884 data points).\n",
            "Split on feature grade_D. (3826, 2058)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (3826 data points).\n",
            "Split on feature grade_E. (1693, 2133)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (1693 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2133 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (2058 data points).\n",
            "Split on feature grade_E. (2058, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (2190 data points).\n",
            "Split on feature grade_D. (2190, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (1048 data points).\n",
            "Split on feature emp_length_5 years. (969, 79)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (969 data points).\n",
            "Split on feature grade_C. (969, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (79 data points).\n",
            "Split on feature home_ownership_MORTGAGE. (34, 45)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (34 data points).\n",
            "Split on feature grade_C. (34, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (45 data points).\n",
            "Split on feature grade_C. (45, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (101 data points).\n",
            "Split on feature emp_length_0. (96, 5)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (96 data points).\n",
            "Split on feature emp_length_< 1 year. (85, 11)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (85 data points).\n",
            "Split on feature grade_B. (85, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (11 data points).\n",
            "Split on feature grade_B. (11, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (5 data points).\n",
            "Split on feature grade_B. (5, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (28001 data points).\n",
            "Split on feature grade_D. (23300, 4701)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (23300 data points).\n",
            "Split on feature grade_E. (22024, 1276)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (22024 data points).\n",
            "Split on feature grade_F. (21666, 358)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (21666 data points).\n",
            "Split on feature emp_length_0. (20734, 932)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (20734 data points).\n",
            "Split on feature grade_G. (20638, 96)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (20638 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (96 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (932 data points).\n",
            "Split on feature grade_A. (702, 230)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (702 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (230 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (358 data points).\n",
            "Split on feature emp_length_8 years. (347, 11)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (347 data points).\n",
            "Split on feature grade_A. (347, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (11 data points).\n",
            "Split on feature home_ownership_OWN. (9, 2)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (9 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2 data points).\n",
            "Stopping condition 1 reached.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (1276 data points).\n",
            "Split on feature grade_A. (1276, 0)\n",
            "Creating leaf node.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (4701 data points).\n",
            "Split on feature grade_A. (4701, 0)\n",
            "Creating leaf node.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYuAT2NowdsJ",
        "colab_type": "code",
        "outputId": "37ad0c30-c1b9-4c41-c3d3-9c98ad3852cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "my_decision_tree"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is_leaf': False,\n",
              " 'left': {'is_leaf': False,\n",
              "  'left': {'is_leaf': False,\n",
              "   'left': {'is_leaf': False,\n",
              "    'left': {'is_leaf': False,\n",
              "     'left': {'is_leaf': False,\n",
              "      'left': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': -1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'prediction': None,\n",
              "      'right': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': -1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'splitting_feature': 'grade_E'},\n",
              "     'prediction': None,\n",
              "     'right': {'is_leaf': True,\n",
              "      'left': None,\n",
              "      'prediction': -1,\n",
              "      'right': None,\n",
              "      'splitting_feature': None},\n",
              "     'splitting_feature': 'grade_D'},\n",
              "    'prediction': None,\n",
              "    'right': {'is_leaf': True,\n",
              "     'left': None,\n",
              "     'prediction': -1,\n",
              "     'right': None,\n",
              "     'splitting_feature': None},\n",
              "    'splitting_feature': 'grade_C'},\n",
              "   'prediction': None,\n",
              "   'right': {'is_leaf': False,\n",
              "    'left': {'is_leaf': True,\n",
              "     'left': None,\n",
              "     'prediction': -1,\n",
              "     'right': None,\n",
              "     'splitting_feature': None},\n",
              "    'prediction': None,\n",
              "    'right': {'is_leaf': False,\n",
              "     'left': {'is_leaf': True,\n",
              "      'left': None,\n",
              "      'prediction': 1,\n",
              "      'right': None,\n",
              "      'splitting_feature': None},\n",
              "     'prediction': None,\n",
              "     'right': {'is_leaf': True,\n",
              "      'left': None,\n",
              "      'prediction': -1,\n",
              "      'right': None,\n",
              "      'splitting_feature': None},\n",
              "     'splitting_feature': 'home_ownership_MORTGAGE'},\n",
              "    'splitting_feature': 'emp_length_5 years'},\n",
              "   'splitting_feature': 'grade_B'},\n",
              "  'prediction': None,\n",
              "  'right': {'is_leaf': False,\n",
              "   'left': {'is_leaf': False,\n",
              "    'left': {'is_leaf': True,\n",
              "     'left': None,\n",
              "     'prediction': 1,\n",
              "     'right': None,\n",
              "     'splitting_feature': None},\n",
              "    'prediction': None,\n",
              "    'right': {'is_leaf': True,\n",
              "     'left': None,\n",
              "     'prediction': -1,\n",
              "     'right': None,\n",
              "     'splitting_feature': None},\n",
              "    'splitting_feature': 'emp_length_< 1 year'},\n",
              "   'prediction': None,\n",
              "   'right': {'is_leaf': True,\n",
              "    'left': None,\n",
              "    'prediction': -1,\n",
              "    'right': None,\n",
              "    'splitting_feature': None},\n",
              "   'splitting_feature': 'emp_length_0'},\n",
              "  'splitting_feature': 'grade_A'},\n",
              " 'prediction': None,\n",
              " 'right': {'is_leaf': False,\n",
              "  'left': {'is_leaf': False,\n",
              "   'left': {'is_leaf': False,\n",
              "    'left': {'is_leaf': False,\n",
              "     'left': {'is_leaf': False,\n",
              "      'left': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': 1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'prediction': None,\n",
              "      'right': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': -1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'splitting_feature': 'grade_G'},\n",
              "     'prediction': None,\n",
              "     'right': {'is_leaf': False,\n",
              "      'left': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': -1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'prediction': None,\n",
              "      'right': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': 1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'splitting_feature': 'grade_A'},\n",
              "     'splitting_feature': 'emp_length_0'},\n",
              "    'prediction': None,\n",
              "    'right': {'is_leaf': False,\n",
              "     'left': {'is_leaf': True,\n",
              "      'left': None,\n",
              "      'prediction': -1,\n",
              "      'right': None,\n",
              "      'splitting_feature': None},\n",
              "     'prediction': None,\n",
              "     'right': {'is_leaf': False,\n",
              "      'left': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': 1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'prediction': None,\n",
              "      'right': {'is_leaf': True,\n",
              "       'left': None,\n",
              "       'prediction': -1,\n",
              "       'right': None,\n",
              "       'splitting_feature': None},\n",
              "      'splitting_feature': 'home_ownership_OWN'},\n",
              "     'splitting_feature': 'emp_length_8 years'},\n",
              "    'splitting_feature': 'grade_F'},\n",
              "   'prediction': None,\n",
              "   'right': {'is_leaf': True,\n",
              "    'left': None,\n",
              "    'prediction': -1,\n",
              "    'right': None,\n",
              "    'splitting_feature': None},\n",
              "   'splitting_feature': 'grade_E'},\n",
              "  'prediction': None,\n",
              "  'right': {'is_leaf': True,\n",
              "   'left': None,\n",
              "   'prediction': -1,\n",
              "   'right': None,\n",
              "   'splitting_feature': None},\n",
              "  'splitting_feature': 'grade_D'},\n",
              " 'splitting_feature': 'term_ 36 months'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX55TIjUwrnO",
        "colab_type": "text"
      },
      "source": [
        "### Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XH1OyGpwo5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(tree, x, annotate = False):   \n",
        "    # if the node is a leaf node.\n",
        "    if tree['is_leaf']:\n",
        "        if annotate: \n",
        "            print(f\"At leaf, predicting {tree['prediction']}\")\n",
        "        return tree['prediction'] \n",
        "    else:\n",
        "        # split on feature.\n",
        "        split_feature_value = x[tree['splitting_feature']]\n",
        "        if annotate: \n",
        "            print (f\"Split on {tree['splitting_feature']} = {split_feature_value}\")\n",
        "        if split_feature_value == 0:\n",
        "            return classify(tree['left'], x, annotate)\n",
        "        else:\n",
        "            return classify(tree['right'], x, annotate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf97pnqewt5X",
        "colab_type": "code",
        "outputId": "d1bc583e-8687-4e79-e6f7-f8ac24af0be8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "test_data.iloc[0]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safe_loans                -1\n",
              "grade_A                    0\n",
              "grade_B                    0\n",
              "grade_C                    0\n",
              "grade_D                    1\n",
              "grade_E                    0\n",
              "grade_F                    0\n",
              "grade_G                    0\n",
              "term_ 36 months            0\n",
              "term_ 60 months            1\n",
              "home_ownership_MORTGAGE    0\n",
              "home_ownership_OTHER       0\n",
              "home_ownership_OWN         0\n",
              "home_ownership_RENT        1\n",
              "emp_length_0               0\n",
              "emp_length_1 year          0\n",
              "emp_length_10+ years       0\n",
              "emp_length_2 years         1\n",
              "emp_length_3 years         0\n",
              "emp_length_4 years         0\n",
              "emp_length_5 years         0\n",
              "emp_length_6 years         0\n",
              "emp_length_7 years         0\n",
              "emp_length_8 years         0\n",
              "emp_length_9 years         0\n",
              "emp_length_< 1 year        0\n",
              "Name: 24, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btd8379gwyyi",
        "colab_type": "code",
        "outputId": "9992d4bc-2534-4082-9155-900ac5c0d049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Predicted class: %s ' % classify(my_decision_tree, test_data.iloc[0]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: -1 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwdwynaxw7_L",
        "colab_type": "code",
        "outputId": "c9fa8f96-d16e-41f8-9888-1b7cc752048f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "classify(my_decision_tree, test_data.iloc[0], annotate=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split on term_ 36 months = 0\n",
            "Split on grade_A = 0\n",
            "Split on grade_B = 0\n",
            "Split on grade_C = 0\n",
            "Split on grade_D = 1\n",
            "At leaf, predicting -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tAEntAvxTRm",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25FG1d5HNXXq",
        "colab_type": "text"
      },
      "source": [
        "$ classification \\ error=  \\dfrac{number \\ of \\ mistakes}{total \\ number \\ of \\ examples} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ZPwXQUxNe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_classification_error(tree, data):\n",
        "    \n",
        "    # Applying the classify(tree, x) to each row of data\n",
        "    prediction = []\n",
        "    for i in range(len(data)):\n",
        "        prediction.append(classify(tree, data.iloc[i]))  \n",
        "\n",
        "    # Calculate the classification error\n",
        "    num_of_mistakes = (prediction != data[target]).sum()/float(len(data))\n",
        "    return num_of_mistakes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKyFLR2pxVrh",
        "colab_type": "code",
        "outputId": "59b60138-958e-4986-f12a-60ef57502f68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluate_classification_error(my_decision_tree, test_data)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3837785437311504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXIj3MrJZIyJ",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpmAQY7hZg7_",
        "colab_type": "text"
      },
      "source": [
        "**Early stopping**\n",
        "\n",
        "1. Maximum depth. *(set by parameter `max_depth`)*\n",
        "2. Minimum node size. *(set by parameter `min_node_size`)*\n",
        "3. Minimum gain in error reduction *(set by parameter `min_error_reduction`)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unc7wweqZv8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 already implemented"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNAwaOvxXz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2\n",
        "def reached_minimum_node_size(data, min_node_size):\n",
        "    # Return True if the number of data points is less than or equal to the minimum node size.    \n",
        "    if len(data) <= min_node_size:\n",
        "        return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdQ1M2dIZVvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#3\n",
        "def error_reduction(error_before_split, error_after_split):\n",
        "    # Return the error before the split minus the error after the split.\n",
        "    error = error_before_split - error_after_split\n",
        "    return error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8Jv7JVSOKm1",
        "colab_type": "text"
      },
      "source": [
        "Function with all 3 early stopping conditions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknrWHfYZXvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decision_tree_create_overfit(data, features, target, current_depth = 0, max_depth = 10, \n",
        "                                 min_node_size=1, min_error_reduction=0.0):\n",
        "    \n",
        "    remaining_features = features.copy()\n",
        "    \n",
        "    target_values = data[target]\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(f\"Subtree, depth = {current_depth} ({len(target_values)} data points).\")\n",
        "    \n",
        "    \n",
        "    # Stopping condition 1: All nodes are of the same type.\n",
        "    if intermediate_node_num_mistakes(target_values) == 0:\n",
        "        print(\"Stopping condition 1 reached. All data points have the same target value.\")\n",
        "        return create_leaf(target_values)\n",
        "    \n",
        "    # Stopping condition 2: No more features to split on.\n",
        "    if remaining_features.empty:\n",
        "        print(\"Stopping condition 2 reached. No remaining features.\")\n",
        "        return create_leaf(target_values)    \n",
        "    \n",
        "    # Early stopping condition 1: Reached max depth limit.\n",
        "    if current_depth >= max_depth:\n",
        "        print(\"Early stopping condition 1 reached. Reached maximum depth.\")\n",
        "        return create_leaf(target_values)\n",
        "    \n",
        "    # Early stopping condition 2: Reached the minimum node size.\n",
        "    if reached_minimum_node_size(data, min_node_size):\n",
        "        print(\"Early stopping condition 2 reached. Reached minimum node size.\")\n",
        "        return create_leaf(target_values)\n",
        "    \n",
        "    # Find the best splitting feature\n",
        "    splitting_feature = best_splitting_feature(data, features, target)\n",
        "    \n",
        "    # Split on the best feature \n",
        "    left_split = data[data[splitting_feature] == 0]\n",
        "    right_split = data[data[splitting_feature] == 1]\n",
        "    \n",
        "    # Early stopping condition 3: Minimum error reduction\n",
        "    error_before_split = intermediate_node_num_mistakes(target_values)/float(len(data))\n",
        "    \n",
        "    # Calculate the error after splitting (in both groups divided by the total number of examples)\n",
        "    left_mistakes = intermediate_node_num_mistakes(left_split[target])\n",
        "    right_mistakes = intermediate_node_num_mistakes(right_split[target])\n",
        "    error_after_split = (left_mistakes + right_mistakes) / float(len(data))\n",
        "    \n",
        "    # If the error reduction is less than or equal to min_error_reduction, return a leaf\n",
        "    if error_reduction(error_before_split, error_after_split) <= min_error_reduction:\n",
        "        print(\"Early stopping condition 3 reached. Minimum error reduction.\")\n",
        "        return create_leaf(target_values)\n",
        "    \n",
        "    remaining_features.drop(splitting_feature, axis=1, inplace=True)\n",
        "    print(f\"Split on feature {splitting_feature}. ({len(left_split)}, {len(right_split)})\")\n",
        "    \n",
        "\n",
        "    # Repeat (recurse) on left and right subtrees\n",
        "    left_tree = decision_tree_create_overfit(left_split, remaining_features, target, current_depth + 1, \n",
        "                                             max_depth, min_node_size, min_error_reduction)        \n",
        "    right_tree = decision_tree_create_overfit(right_split, remaining_features, target, current_depth + 1,\n",
        "                                              max_depth, min_node_size, min_error_reduction) \n",
        "    \n",
        "    return {'is_leaf'          : False, \n",
        "            'prediction'       : None,\n",
        "            'splitting_feature': splitting_feature,\n",
        "            'left'             : left_tree, \n",
        "            'right'            : right_tree}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3_m6-7dcDDG",
        "colab_type": "text"
      },
      "source": [
        "#### Fitting the Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuGZ2R35cC2U",
        "colab_type": "code",
        "outputId": "d207a2f8-c274-4460-d740-38defeaa25c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "my_decision_tree_new = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 100, min_error_reduction=0.0)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 0 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (9223 data points).\n",
            "Split on feature grade_A. (9122, 101)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9122 data points).\n",
            "Early stopping condition 3 reached. Minimum error reduction.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (101 data points).\n",
            "Split on feature emp_length_0. (96, 5)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (96 data points).\n",
            "Early stopping condition 2 reached. Reached minimum node size.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (5 data points).\n",
            "Early stopping condition 2 reached. Reached minimum node size.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (28001 data points).\n",
            "Split on feature grade_D. (23300, 4701)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (23300 data points).\n",
            "Split on feature grade_E. (22024, 1276)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (22024 data points).\n",
            "Split on feature grade_F. (21666, 358)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (21666 data points).\n",
            "Split on feature emp_length_0. (20734, 932)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (20734 data points).\n",
            "Split on feature grade_G. (20638, 96)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (20638 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (96 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (932 data points).\n",
            "Split on feature grade_A. (702, 230)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (702 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (230 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (358 data points).\n",
            "Split on feature emp_length_8 years. (347, 11)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (347 data points).\n",
            "Early stopping condition 3 reached. Minimum error reduction.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (11 data points).\n",
            "Early stopping condition 2 reached. Reached minimum node size.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (1276 data points).\n",
            "Early stopping condition 3 reached. Minimum error reduction.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (4701 data points).\n",
            "Early stopping condition 3 reached. Minimum error reduction.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSDTRLXscP3o",
        "colab_type": "text"
      },
      "source": [
        "Let's now train a tree model ignoring early stopping conditions 2 and 3. To ignore these conditions, we set `min_node_size=0` and `min_error_reduction=-1` *(a negative value*)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEGIn1fAbQMr",
        "colab_type": "code",
        "outputId": "09f39b2c-9706-4021-bd2c-02ae6cb35bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "my_decision_tree_old = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=-1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 0 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (9223 data points).\n",
            "Split on feature grade_A. (9122, 101)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9122 data points).\n",
            "Split on feature grade_B. (8074, 1048)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (8074 data points).\n",
            "Split on feature grade_C. (5884, 2190)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (5884 data points).\n",
            "Split on feature grade_D. (3826, 2058)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (3826 data points).\n",
            "Split on feature grade_E. (1693, 2133)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (1693 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2133 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (2058 data points).\n",
            "Split on feature grade_E. (2058, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2058 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (2190 data points).\n",
            "Split on feature grade_D. (2190, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (2190 data points).\n",
            "Split on feature grade_E. (2190, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2190 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (1048 data points).\n",
            "Split on feature emp_length_5 years. (969, 79)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (969 data points).\n",
            "Split on feature grade_C. (969, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (969 data points).\n",
            "Split on feature grade_D. (969, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (969 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (79 data points).\n",
            "Split on feature home_ownership_MORTGAGE. (34, 45)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (34 data points).\n",
            "Split on feature grade_C. (34, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (34 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (45 data points).\n",
            "Split on feature grade_C. (45, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (45 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (101 data points).\n",
            "Split on feature emp_length_0. (96, 5)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (96 data points).\n",
            "Split on feature emp_length_< 1 year. (85, 11)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (85 data points).\n",
            "Split on feature grade_B. (85, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (85 data points).\n",
            "Split on feature grade_C. (85, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (85 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (11 data points).\n",
            "Split on feature grade_B. (11, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (11 data points).\n",
            "Split on feature grade_C. (11, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (11 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (5 data points).\n",
            "Split on feature grade_B. (5, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (5 data points).\n",
            "Split on feature grade_C. (5, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (5 data points).\n",
            "Split on feature grade_D. (5, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (5 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (28001 data points).\n",
            "Split on feature grade_D. (23300, 4701)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (23300 data points).\n",
            "Split on feature grade_E. (22024, 1276)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (22024 data points).\n",
            "Split on feature grade_F. (21666, 358)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (21666 data points).\n",
            "Split on feature emp_length_0. (20734, 932)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (20734 data points).\n",
            "Split on feature grade_G. (20638, 96)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (20638 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (96 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (932 data points).\n",
            "Split on feature grade_A. (702, 230)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (702 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (230 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (358 data points).\n",
            "Split on feature emp_length_8 years. (347, 11)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (347 data points).\n",
            "Split on feature grade_A. (347, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (347 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (11 data points).\n",
            "Split on feature home_ownership_OWN. (9, 2)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (9 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (2 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (1276 data points).\n",
            "Split on feature grade_A. (1276, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (1276 data points).\n",
            "Split on feature grade_B. (1276, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (1276 data points).\n",
            "Split on feature grade_C. (1276, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (1276 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (4701 data points).\n",
            "Split on feature grade_A. (4701, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (4701 data points).\n",
            "Split on feature grade_B. (4701, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (4701 data points).\n",
            "Split on feature grade_C. (4701, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (4701 data points).\n",
            "Split on feature grade_E. (4701, 0)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (4701 data points).\n",
            "Early stopping condition 1 reached. Reached maximum depth.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 6 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 5 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 4 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (0 data points).\n",
            "Stopping condition 1 reached. All data points have the same target value.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QHPi3TpcSQI",
        "colab_type": "code",
        "outputId": "4063c139-bc80-48a9-a15d-61daec160030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "test_data.iloc[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "safe_loans                -1\n",
              "grade_A                    0\n",
              "grade_B                    0\n",
              "grade_C                    0\n",
              "grade_D                    1\n",
              "grade_E                    0\n",
              "grade_F                    0\n",
              "grade_G                    0\n",
              "term_ 36 months            0\n",
              "term_ 60 months            1\n",
              "home_ownership_MORTGAGE    0\n",
              "home_ownership_OTHER       0\n",
              "home_ownership_OWN         0\n",
              "home_ownership_RENT        1\n",
              "emp_length_0               0\n",
              "emp_length_1 year          0\n",
              "emp_length_10+ years       0\n",
              "emp_length_2 years         1\n",
              "emp_length_3 years         0\n",
              "emp_length_4 years         0\n",
              "emp_length_5 years         0\n",
              "emp_length_6 years         0\n",
              "emp_length_7 years         0\n",
              "emp_length_8 years         0\n",
              "emp_length_9 years         0\n",
              "emp_length_< 1 year        0\n",
              "Name: 24, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpxqN-Vycgxu",
        "colab_type": "code",
        "outputId": "7befd597-66eb-4881-b48b-260067c5412b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(f'Predicted class: {classify(my_decision_tree_new, test_data.iloc[0])}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: -1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g7bKBwMcmpZ",
        "colab_type": "code",
        "outputId": "77bf4d99-c65f-478b-d681-78503d39b3ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "classify(my_decision_tree_new, test_data.iloc[0], annotate = True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split on term_ 36 months = 0\n",
            "Split on grade_A = 0\n",
            "At leaf, predicting -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWsBdUKQcv-8",
        "colab_type": "text"
      },
      "source": [
        "The prediction path for the decision without overfitting *(tree model ignoring early stopping conditions 2 and 3)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_BR0IrycpXp",
        "colab_type": "code",
        "outputId": "e728516f-ff22-4baa-9756-75efcd3180fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "classify(my_decision_tree_old, test_data.iloc[0], annotate = True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split on term_ 36 months = 0\n",
            "Split on grade_A = 0\n",
            "Split on grade_B = 0\n",
            "Split on grade_C = 0\n",
            "Split on grade_D = 1\n",
            "Split on grade_E = 0\n",
            "At leaf, predicting -1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvCrEJD4cyby",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7tog_qPbh3-",
        "colab_type": "text"
      },
      "source": [
        "Tree with overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWs0puiocs0F",
        "colab_type": "code",
        "outputId": "5604ecf9-1c1e-4f61-b4d4-c2ce9553ce9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluate_classification_error(my_decision_tree_new, test_data)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38367083153813014"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNOrsRtbglm",
        "colab_type": "text"
      },
      "source": [
        "Tree without overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJQOJS0PbWCz",
        "colab_type": "code",
        "outputId": "37b92d7d-a8dd-408f-bc9f-722b125980a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "evaluate_classification_error(my_decision_tree_old, test_data)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3837785437311504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh9HYe14g4_c",
        "colab_type": "text"
      },
      "source": [
        "## Complexity of the Tree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFuVKodxg-XN",
        "colab_type": "text"
      },
      "source": [
        "$ complexity(T) $ = number of leaves in the tree T"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hPWM-1hg7_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_leaves(tree):\n",
        "    if tree['is_leaf']:\n",
        "        return 1\n",
        "    return count_leaves(tree['left']) + count_leaves(tree['right'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEpiyFC4c-w7",
        "colab_type": "text"
      },
      "source": [
        "## Fitting Many Different Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE2oKb1MicrQ",
        "colab_type": "text"
      },
      "source": [
        "***NOTE:*** We assume validation set as test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-3ZFpJkip6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_data = test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fox8JG1ChKd-",
        "colab_type": "text"
      },
      "source": [
        "Different `max_depth` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-47dHc9acz6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_1 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 2, \n",
        "                                min_node_size = 0, min_error_reduction=-1)\n",
        "model_2 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=-1)\n",
        "model_3 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 14, \n",
        "                                min_node_size = 0, min_error_reduction=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEn-ijIlc__W",
        "colab_type": "code",
        "outputId": "c50c0a68-889c-47f1-84ca-51edd7389b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Training data, classification error (model 1):\", evaluate_classification_error(model_1, train_data))\n",
        "print(\"Training data, classification error (model 2):\", evaluate_classification_error(model_2, train_data))\n",
        "print(\"Training data, classification error (model 3):\", evaluate_classification_error(model_3, train_data))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data, classification error (model 1): 0.40003761014399314\n",
            "Training data, classification error (model 2): 0.38185041908446166\n",
            "Training data, classification error (model 3): 0.37446271222866967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd7KlV_YdFdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "aea811d9-c4fe-4501-e7b3-a72da611d537"
      },
      "source": [
        "print (\"Validation data, classification error (model 1):\", evaluate_classification_error(model_1, validation_data))\n",
        "print (\"Validation data, classification error (model 2):\", evaluate_classification_error(model_2, validation_data))\n",
        "print (\"Validation data, classification error (model 3):\", evaluate_classification_error(model_3, validation_data))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation data, classification error (model 1): 0.3981042654028436\n",
            "Validation data, classification error (model 2): 0.3837785437311504\n",
            "Validation data, classification error (model 3): 0.38000861697544164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA0gXpEJgwlw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1ee2ca89-55b6-47a4-e99f-6c0a62c69475"
      },
      "source": [
        "print(\"Number of nodes (model 1):\", count_leaves(model_1))\n",
        "print(\"Number of nodes (model 2):\", count_leaves(model_2))\n",
        "print(\"Number of nodes (model 3):\", count_leaves(model_3))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes (model 1): 4\n",
            "Number of nodes (model 2): 41\n",
            "Number of nodes (model 3): 341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a061IcuoQKRO",
        "colab_type": "text"
      },
      "source": [
        "Different `min_error_reduction` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e42SIMFXg28u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_4 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=-1)\n",
        "model_5 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=0)\n",
        "model_6 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeDvQlkShblJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "c0c58285-9d6e-4635-99d8-3f511ba27d8a"
      },
      "source": [
        "print (\"Validation data, classification error (model 4):\", evaluate_classification_error(model_4, validation_data))\n",
        "print (\"Validation data, classification error (model 5):\", evaluate_classification_error(model_5, validation_data))\n",
        "print (\"Validation data, classification error (model 6):\", evaluate_classification_error(model_6, validation_data))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation data, classification error (model 4): 0.3837785437311504\n",
            "Validation data, classification error (model 5): 0.3837785437311504\n",
            "Validation data, classification error (model 6): 0.503446790176648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8a_ua5_hjSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6d63c5d2-8bcc-4a7f-8195-0b7e416f575a"
      },
      "source": [
        "print( \"Number of nodes (model 4):\", count_leaves(model_4))\n",
        "print (\"Number of nodes (model 5):\", count_leaves(model_5))\n",
        "print (\"Number of nodes (model 6):\", count_leaves(model_6))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes (model 4): 41\n",
            "Number of nodes (model 5): 13\n",
            "Number of nodes (model 6): 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CObAatL2iA2a",
        "colab_type": "text"
      },
      "source": [
        "Different `min_node_size` values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvgSamlph2SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_7 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 0, min_error_reduction=-1)\n",
        "model_8 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 2000, min_error_reduction=-1)\n",
        "model_9 = decision_tree_create_overfit(train_data, features, 'safe_loans', max_depth = 6, \n",
        "                                min_node_size = 50000, min_error_reduction=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNSsQLbziE-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4154edc6-aaf7-4604-db9d-3fb25df650c4"
      },
      "source": [
        "print (\"Validation data, classification error (model 7):\", evaluate_classification_error(model_7, validation_data))\n",
        "print (\"Validation data, classification error (model 8):\", evaluate_classification_error(model_8, validation_data))\n",
        "print (\"Validation data, classification error (model 9):\", evaluate_classification_error(model_9, validation_data))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation data, classification error (model 7): 0.3837785437311504\n",
            "Validation data, classification error (model 8): 0.38453252908229213\n",
            "Validation data, classification error (model 9): 0.503446790176648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOXFSpLFiKrM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "56821d56-e279-47e8-87d5-925e783454e4"
      },
      "source": [
        "print( \"Number of nodes (model 7):\", count_leaves(model_7))\n",
        "print (\"Number of nodes (model 8):\", count_leaves(model_8))\n",
        "print (\"Number of nodes (model 9):\", count_leaves(model_9))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes (model 7): 41\n",
            "Number of nodes (model 8): 19\n",
            "Number of nodes (model 9): 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSzQAPO9QpCP",
        "colab_type": "text"
      },
      "source": [
        "## Weighted Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y_LNYIJpnwj",
        "colab_type": "text"
      },
      "source": [
        "$ E(α,\\hat{y})= \\dfrac{\\sum α_i × 1[y_i≠\\hat{y_i}]} {\\sum α_i} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWonGgmsotsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
        "\n",
        "    # Sum the weights of all entries with label +1 and -1\n",
        "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
        "    weighted_mistakes_all_negative = total_weight_positive\n",
        "\n",
        "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
        "    weighted_mistakes_all_positive = total_weight_negative\n",
        "    \n",
        "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
        "    if weighted_mistakes_all_positive <= weighted_mistakes_all_negative:        \n",
        "        return (weighted_mistakes_all_positive, +1)\n",
        "    else:        \n",
        "        return (weighted_mistakes_all_negative, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFqWWZnFQ0xI",
        "colab_type": "text"
      },
      "source": [
        "The function **best_splitting_feature** should now accept an extra parameter `data_weights` to take account of weights of data points.\n",
        "Instead of computing the number of mistakes in the left and right side of the split, we compute the weight of mistakes for both sides, add up the two weights, and divide it by the total weight of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxXNH69upJh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def best_splitting_feature_weighted(data, features, target, data_weights):\n",
        "    \n",
        "    best_feature = None\n",
        "    best_error = float('+inf') \n",
        "    num_points = float(len(data))\n",
        "\n",
        "    for feature in features:\n",
        "        \n",
        "        # split data points according to 0 or 1\n",
        "        left_split = data[data[feature] == 0]\n",
        "        right_split = data[data[feature] == 1]\n",
        "        \n",
        "        # Apply the same filtering to data_weights\n",
        "        left_data_weights = data_weights[data[feature] == 0]\n",
        "        right_data_weights = data_weights[data[feature] == 1]\n",
        "                    \n",
        "        # Calculate the weight of mistakes for left and right sides\n",
        "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
        "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
        "        \n",
        "        # Compute weighted error by computing\n",
        "        error = (left_weighted_mistakes + right_weighted_mistakes)/(sum(left_data_weights) + sum(right_data_weights))\n",
        "        \n",
        "        #Store the feature and the best error\n",
        "        if error < best_error:\n",
        "            best_feature = feature\n",
        "            best_error = error    \n",
        "\n",
        "    return best_feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1kPuukqffq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_leaf_weighted(target_values, data_weights):\n",
        "    \n",
        "    # Create a leaf node\n",
        "    leaf = {'splitting_feature' : None,\n",
        "            'is_leaf': True}\n",
        "    \n",
        "    # Computed weight of mistakes.\n",
        "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
        "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
        "    leaf['prediction'] = best_class\n",
        "    \n",
        "    return leaf "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYPsL2A3ROiT",
        "colab_type": "text"
      },
      "source": [
        "**Stopping Conditions:**\n",
        "\n",
        "1. All data points in a node are from the same class.\n",
        "2. No more features to split on.\n",
        "3. Stop growing the tree when the tree depth reaches `max_depth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vTSdGpQqlJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
        "    remaining_features = features.copy()\n",
        "    target_values = data[target]\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(f\"Subtree, depth = {current_depth} ({len(target_values)} data points).\")\n",
        "    \n",
        "    # Stopping condition 1. Error is 0.\n",
        "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
        "        print(\"Stopping condition 1 reached.\")\n",
        "        return create_leaf_weighted(target_values, data_weights)\n",
        "    \n",
        "    # Stopping condition 2. No more features.\n",
        "    if remaining_features.empty:\n",
        "        print(\"Stopping condition 2 reached.\")\n",
        "        return create_leaf_weighted(target_values, data_weights)    \n",
        "    \n",
        "    # Stopping condition 3. Limit tree depth.\n",
        "    if current_depth > max_depth:\n",
        "        print(\"Reached maximum depth. Stopping for now.\")\n",
        "        return create_leaf_weighted(target_values, data_weights)\n",
        "    \n",
        "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
        "    splitting_feature = best_splitting_feature_weighted(data, features, target, data_weights)\n",
        "    remaining_features.drop(splitting_feature, axis=1, inplace=True)\n",
        "        \n",
        "    left_split = data[data[splitting_feature] == 0]\n",
        "    right_split = data[data[splitting_feature] == 1]\n",
        "    \n",
        "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
        "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
        "    \n",
        "    print (f\"Split on feature {splitting_feature}. ({len(left_split)}, {len(right_split)})\")\n",
        "    \n",
        "    # Create a leaf node if the split is perfect\n",
        "    if len(left_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf_weighted(left_split[target], data_weights)\n",
        "    if len(right_split) == len(data):\n",
        "        print(\"Creating leaf node.\")\n",
        "        return create_leaf_weighted(right_split[target], data_weights)\n",
        "    \n",
        "    # Repeat (recurse) on left and right subtrees\n",
        "    left_tree = weighted_decision_tree_create(\n",
        "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
        "    right_tree = weighted_decision_tree_create(\n",
        "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
        "    \n",
        "    return {'is_leaf'          : False, \n",
        "            'prediction'       : None,\n",
        "            'splitting_feature': splitting_feature,\n",
        "            'left'             : left_tree, \n",
        "            'right'            : right_tree}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMGt4L7OR6i1",
        "colab_type": "text"
      },
      "source": [
        "Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEGhffjOSHSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3eb19281-be1f-4603-bf9b-2b883fd52386"
      },
      "source": [
        "# Assign weights\n",
        "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
        "\n",
        "# Train a weighted decision tree model.\n",
        "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
        "                         example_data_weights, max_depth=2)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature home_ownership_RENT. (20514, 16710)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (20514 data points).\n",
            "Split on feature grade_F. (19613, 901)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (19613 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (901 data points).\n",
            "Stopping condition 1 reached.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (16710 data points).\n",
            "Split on feature grade_D. (13315, 3395)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (13315 data points).\n",
            "Stopping condition 1 reached.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 3 (3395 data points).\n",
            "Stopping condition 1 reached.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKSKEXI_Srmv",
        "colab_type": "text"
      },
      "source": [
        "Now, we will compute the classification error on the subset_20, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l69LBxQqSpsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a37e7c1-7b34-4826-e7f1-ef269e1d03a9"
      },
      "source": [
        "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
        "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pROmPD2DStNa",
        "colab_type": "text"
      },
      "source": [
        "Compare the classification error of the model small_data_decision_tree_subset_20 on the entire test set train_data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_27Z7x2SvhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b9b317d1-3aa0-4d10-d71e-18e155ad0889"
      },
      "source": [
        "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.48124865678057166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qden1OHnS50I",
        "colab_type": "text"
      },
      "source": [
        "The model small_data_decision_tree_subset_20 performs a lot better on subset_20 than on train_data.\n",
        "\n",
        "The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
        "The points with zero weights are basically ignored during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNKf16KworJB",
        "colab_type": "text"
      },
      "source": [
        "## Boosting a Decision Stump"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwa-QspLrf32",
        "colab_type": "text"
      },
      "source": [
        "Adaboost (on decision stumps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERLCXrOErigl",
        "colab_type": "text"
      },
      "source": [
        "$ \\hat{w}_t = \\dfrac{1}{2} \\dfrac{ln(1−E(α,\\hat{y}))}{E(α,\\hat{y})} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G-deXFo-BP8",
        "colab_type": "text"
      },
      "source": [
        "Re-compute & Normalize weights $ α_j $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668UjOeZrlue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
        "\n",
        "    # start with unweighted data\n",
        "    alpha = np.array([1.]*len(data))\n",
        "    weights = []\n",
        "    tree_stumps = []\n",
        "    target_values = data[target]\n",
        "    \n",
        "    for t in range(num_tree_stumps):\n",
        "        print ('=====================================================')\n",
        "        print ('Adaboost Iteration:', t)\n",
        "        print ('=====================================================' ) \n",
        "\n",
        "        # Learn a weighted decision tree stump. max_depth=1\n",
        "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
        "        tree_stumps.append(tree_stump)\n",
        "        \n",
        "        # Make predictions\n",
        "        predictions = []\n",
        "        for i in range(len(data)):\n",
        "            predictions.append(classify(tree_stump, data.iloc[i]))  \n",
        "        \n",
        "        # Produce a Boolean array indicating whether, each data point was correctly classified\n",
        "        is_correct = predictions == target_values\n",
        "        is_wrong   = predictions != target_values\n",
        "        \n",
        "        # Compute weighted error\n",
        "        weighted_error = sum(alpha[is_wrong])/sum(alpha)\n",
        "        \n",
        "        # Compute model coefficient using weighted error\n",
        "        weight = 1./2. * np.log((1-weighted_error)/weighted_error)\n",
        "        weights.append(weight)\n",
        "        \n",
        "        # Adjust weights on data point\n",
        "        adjustment = is_correct.apply(lambda is_correct : np.exp(-weight) if is_correct else np.exp(weight))\n",
        "        \n",
        "        # Scale alpha by multiplying by adjustment and normalize data points weights \n",
        "        alpha = (alpha*adjustment)/float(sum(alpha))\n",
        "\n",
        "    return weights, tree_stumps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKEH3QyArfL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "c177dad6-aced-42d5-8d27-651d3874588d"
      },
      "source": [
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====================================================\n",
            "Adaboost Iteration: 0\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9223 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (28001 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 1\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jJPp8PJr0lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5959128b-3a01-49d3-f76f-a6971b5d4d0f"
      },
      "source": [
        "print(stump_weights)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.15802933659263743, 0.17682363293605327]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-XYcZ1K067d",
        "colab_type": "text"
      },
      "source": [
        "### Training a Boosted Ensemble of 10 Stumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU7_PaPe014y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94ba69ab-e7ce-4efa-b752-ebf4c2c1259a"
      },
      "source": [
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=10)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====================================================\n",
            "Adaboost Iteration: 0\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9223 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (28001 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 1\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 2\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_D. (30465, 6759)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (30465 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (6759 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 3\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (19846 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (17378 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 4\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_B. (26858, 10366)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (26858 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (10366 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 5\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_E. (33815, 3409)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (33815 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (3409 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 6\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 7\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_F. (35512, 1712)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35512 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1712 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 8\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 9\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sTdJ8Ic9DuS",
        "colab_type": "text"
      },
      "source": [
        "#### Making Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XphNKG2U1Be5",
        "colab_type": "text"
      },
      "source": [
        "$ \\hat{y} = sign(\\sum_{t=1}^{T} \\hat{w} f_t(x)) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rseipqNU08ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_adaboost(stump_weights, tree_stumps, data):\n",
        "    scores = np.array([0.]*len(data))\n",
        "    \n",
        "    for i, tree_stump in enumerate(tree_stumps):\n",
        "\n",
        "        predictions = []\n",
        "        for j in range(len(data)):\n",
        "            predictions.append(classify(tree_stump, data.iloc[j]))\n",
        "        \n",
        "        # Accumulate predictions on scores array\n",
        "        scores += (stump_weights[i] * np.asarray(predictions))\n",
        "        \n",
        "    return pd.Series(scores).apply(lambda score : +1 if score > 0 else -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC1_zVAP1I3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "b4426faa-d5dc-42ba-b0d6-6b16cbe75bb9"
      },
      "source": [
        "stump_weights"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.15802933659263743,\n",
              " 0.17682363293605327,\n",
              " 0.09311888971195705,\n",
              " 0.0728888552581495,\n",
              " 0.06706306914131716,\n",
              " 0.06456916961613322,\n",
              " 0.05456055779221647,\n",
              " 0.04351093673354489,\n",
              " 0.028988711500059067,\n",
              " 0.0259625096913776]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKI0tp4k1Dfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7a20883-6ff1-4039-f019-40d6dad93a16"
      },
      "source": [
        "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
        "accuracy = (predictions == test_data[target].values).sum()/len(test_data)\n",
        "print('Accuracy of 10-component ensemble =', accuracy )"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of 10-component ensemble = 0.6203145196036192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIFk9pO42-V2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "541b9c4e-5da6-4e9f-af2e-066b875b2031"
      },
      "source": [
        "sns.distplot(test_data[target], hist=False, color=\"r\", label=\"Actual Value\")\n",
        "sns.distplot(predictions, hist=False, color=\"b\", label=\"Fitted Values\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1195de8e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LuR7unYbHq2",
        "colab_type": "text"
      },
      "source": [
        "## Performance Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81M_hKXxbVIK",
        "colab_type": "text"
      },
      "source": [
        "Training a Boosted Ensemble of 30 Stumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxDUi-OtZi3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b0e2c94-eaa9-46a9-c98c-169779324bf8"
      },
      "source": [
        "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=30)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=====================================================\n",
            "Adaboost Iteration: 0\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9223 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (28001 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 1\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 2\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_D. (30465, 6759)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (30465 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (6759 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 3\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (19846 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (17378 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 4\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_B. (26858, 10366)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (26858 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (10366 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 5\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_E. (33815, 3409)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (33815 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (3409 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 6\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 7\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_F. (35512, 1712)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35512 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1712 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 8\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 9\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 10\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_D. (30465, 6759)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (30465 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (6759 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 11\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_B. (26858, 10366)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (26858 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (10366 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 12\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 13\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_4 years. (34593, 2631)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (34593 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (2631 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 14\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 15\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_C. (27812, 9412)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (27812 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9412 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 16\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 17\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_F. (35512, 1712)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35512 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1712 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 18\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature term_ 36 months. (9223, 28001)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9223 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (28001 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 19\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_B. (26858, 10366)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (26858 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (10366 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 20\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 21\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_D. (30465, 6759)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (30465 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (6759 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 22\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_F. (35512, 1712)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35512 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1712 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 23\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_A. (32094, 5130)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (32094 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (5130 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 24\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 25\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_2 years. (33652, 3572)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (33652 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (3572 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 26\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_F. (35512, 1712)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35512 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1712 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 27\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature home_ownership_OWN. (34149, 3075)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (34149 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (3075 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 28\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature emp_length_0. (35781, 1443)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (35781 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (1443 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "=====================================================\n",
            "Adaboost Iteration: 29\n",
            "=====================================================\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 1 (37224 data points).\n",
            "Split on feature grade_C. (27812, 9412)\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (27812 data points).\n",
            "Reached maximum depth. Stopping for now.\n",
            "--------------------------------------------------------------------\n",
            "Subtree, depth = 2 (9412 data points).\n",
            "Reached maximum depth. Stopping for now.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM-yctFEcKvW",
        "colab_type": "text"
      },
      "source": [
        "Computing Training Error at the end of Each Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLK3yd9vbWHt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "3ee2501a-6d10-4e00-9f57-b4f3b73768b4"
      },
      "source": [
        "train_error_all = []\n",
        "for n in range(1, 31):\n",
        "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
        "    error = (predictions != train_data[target].values).sum()/len(train_data)\n",
        "    train_error_all.append(error)\n",
        "    print(f\"Iteration {n}, training error = {train_error_all[n-1]}\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, training error = 0.4216365785514722\n",
            "Iteration 2, training error = 0.4334300451321728\n",
            "Iteration 3, training error = 0.40003761014399314\n",
            "Iteration 4, training error = 0.40003761014399314\n",
            "Iteration 5, training error = 0.3847249086610789\n",
            "Iteration 6, training error = 0.38461745110681284\n",
            "Iteration 7, training error = 0.3827638082957232\n",
            "Iteration 8, training error = 0.38461745110681284\n",
            "Iteration 9, training error = 0.3827638082957232\n",
            "Iteration 10, training error = 0.38448312916398025\n",
            "Iteration 11, training error = 0.3827369439071567\n",
            "Iteration 12, training error = 0.3814474532559639\n",
            "Iteration 13, training error = 0.38152804642166344\n",
            "Iteration 14, training error = 0.3805609284332689\n",
            "Iteration 15, training error = 0.38050719965613583\n",
            "Iteration 16, training error = 0.3782237266279819\n",
            "Iteration 17, training error = 0.378277455405115\n",
            "Iteration 18, training error = 0.37841177734794756\n",
            "Iteration 19, training error = 0.37806254029658287\n",
            "Iteration 20, training error = 0.37876101439931226\n",
            "Iteration 21, training error = 0.37956694605630775\n",
            "Iteration 22, training error = 0.37889533634214484\n",
            "Iteration 23, training error = 0.37889533634214484\n",
            "Iteration 24, training error = 0.37876101439931226\n",
            "Iteration 25, training error = 0.37889533634214484\n",
            "Iteration 26, training error = 0.3789759295078444\n",
            "Iteration 27, training error = 0.379110251450677\n",
            "Iteration 28, training error = 0.37892220073071137\n",
            "Iteration 29, training error = 0.3790296582849774\n",
            "Iteration 30, training error = 0.37873415001074573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMf3kKOveaUO",
        "colab_type": "text"
      },
      "source": [
        "Computing Test Error at the end of Each Iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRMArmhhcaWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "b07ed28b-d220-405a-90ca-db427752767a"
      },
      "source": [
        "test_error_all = []\n",
        "for n in range(1, 31):\n",
        "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
        "    error = (predictions != test_data[target].values).sum()/len(test_data)\n",
        "    test_error_all.append(error)\n",
        "    print(f\"Iteration {n}, test error = {test_error_all[n-1]}\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, test error = 0.4233089185695821\n",
            "Iteration 2, test error = 0.42847910383455406\n",
            "Iteration 3, test error = 0.3981042654028436\n",
            "Iteration 4, test error = 0.3981042654028436\n",
            "Iteration 5, test error = 0.3799009047824214\n",
            "Iteration 6, test error = 0.38000861697544164\n",
            "Iteration 7, test error = 0.3792546316242999\n",
            "Iteration 8, test error = 0.38000861697544164\n",
            "Iteration 9, test error = 0.3792546316242999\n",
            "Iteration 10, test error = 0.3796854803963809\n",
            "Iteration 11, test error = 0.3792546316242999\n",
            "Iteration 12, test error = 0.37796208530805686\n",
            "Iteration 13, test error = 0.3792546316242999\n",
            "Iteration 14, test error = 0.3778543731150366\n",
            "Iteration 15, test error = 0.3785006462731581\n",
            "Iteration 16, test error = 0.3778543731150366\n",
            "Iteration 17, test error = 0.37796208530805686\n",
            "Iteration 18, test error = 0.3778543731150366\n",
            "Iteration 19, test error = 0.37817750969409736\n",
            "Iteration 20, test error = 0.3768849633778544\n",
            "Iteration 21, test error = 0.37753123653597587\n",
            "Iteration 22, test error = 0.3767772511848341\n",
            "Iteration 23, test error = 0.3767772511848341\n",
            "Iteration 24, test error = 0.3768849633778544\n",
            "Iteration 25, test error = 0.3767772511848341\n",
            "Iteration 26, test error = 0.37656182679879363\n",
            "Iteration 27, test error = 0.3764541146057734\n",
            "Iteration 28, test error = 0.3769926755708746\n",
            "Iteration 29, test error = 0.3767772511848341\n",
            "Iteration 30, test error = 0.3767772511848341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcEFga6JegZF",
        "colab_type": "text"
      },
      "source": [
        "**Visualizing** the Training & Test Errors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyOtC6DceWup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "fe62968d-fb16-42f9-cb9d-57658c5e6e1f"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.lineplot(range(1,31), train_error_all, label='Training Error')\n",
        "sns.lineplot(range(1,31), test_error_all,  label='Test Error')\n",
        "plt.title('Performance of Adaboost ensemble')\n",
        "plt.xlabel('Number of Iterations')\n",
        "plt.ylabel('Classification Error')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}